{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "nav_menu": {
        "height": "318px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Aula 4 - Exercício - Leonardo Pacheco",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardo3108/ia025a/blob/main/Exercicios/Aula4-Regressao_Softmax_MNIST_SGD_minibatches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3R2-_YuQT1r"
      },
      "source": [
        "# Regressão Softmax com dados do MNIST utilizando gradiente descendente estocástico por minibatches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfGTqMVQT1u"
      },
      "source": [
        "Este exercicío consiste em treinar um modelo de uma única camada linear no MNIST **sem** usar as seguintes funções do pytorch:\n",
        "\n",
        "- torch.nn.Linear\n",
        "- torch.nn.CrossEntropyLoss\n",
        "- torch.nn.NLLLoss\n",
        "- torch.nn.LogSoftmax\n",
        "- torch.optim.SGD\n",
        "- torch.utils.data.Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNf4RPxQT1w"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:14.033692Z",
          "start_time": "2018-08-21T14:08:11.179981Z"
        },
        "id": "-fLUSHaCQT1x"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixando as seeds"
      ],
      "metadata": {
        "id": "7_Po22b5ykhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "id": "y-7WWWgLyoRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482b1dce-383e-463b-f6d1-83b32e964ab3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe0d03c0ef0>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMUsfJpQT11"
      },
      "source": [
        "## Dataset e dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoQjDs_QT12"
      },
      "source": [
        "### Definição do tamanho do minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:20.282474Z",
          "start_time": "2018-08-21T14:08:20.275450Z"
        },
        "id": "tEQYUr4TQT13"
      },
      "source": [
        "batch_size = 50"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc7Rv_2BQT16"
      },
      "source": [
        "### Carregamento, criação dataset e do dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:10:45.430605Z",
          "start_time": "2018-08-21T14:10:04.953051Z"
        },
        "id": "G0dEKCn-QT17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72efccf-7676-439f-ccd7-07db9d8bdc1c"
      },
      "source": [
        "dataset_dir = '../data/'\n",
        "\n",
        "dataset_train_full = MNIST(dataset_dir, train=True, download=True,\n",
        "                           transform=torchvision.transforms.ToTensor())\n",
        "print(dataset_train_full.data.shape)\n",
        "print(dataset_train_full.targets.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rOy9ntrQT2D"
      },
      "source": [
        "### Usando apenas 1000 amostras do MNIST\n",
        "\n",
        "Neste exercício utilizaremos 1000 amostras de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNF2XjLBWWe7"
      },
      "source": [
        "indices = torch.randperm(len(dataset_train_full))[:1000]\n",
        "dataset_train = torch.utils.data.Subset(dataset_train_full, indices)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSNLD2JyA2e-"
      },
      "source": [
        "# Escreva aqui o equivalente do código abaixo:\n",
        "# loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.batch_size = batch_size\n",
        "        if shuffle:\n",
        "            self.dataset = dataset[torch.randperm(len(dataset))]\n",
        "        else:\n",
        "            self.dataset = dataset\n",
        "        self.batch_number = ceil(len(self.dataset) / self.batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.batch_number\n",
        "    \n",
        "    def __iter__(self):\n",
        "        self.dataset_index = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        batch_size = self.batch_size\n",
        "        if batch_size + self.dataset_index > len(self.dataset):\n",
        "            batch_size = len(self.dataset) - self.dataset_index\n",
        "            if self.dataset_index >= len(self.dataset):\n",
        "                raise StopIteration\n",
        "        for batch_index in range(batch_size):\n",
        "            data, target = self.dataset[batch_index + self.dataset_index]\n",
        "            if batch_index == 0:\n",
        "                batch_data = data\n",
        "                batch_targets = torch.tensor([target])\n",
        "            else:\n",
        "                batch_data = torch.cat((batch_data, data))\n",
        "                batch_targets = torch.cat((batch_targets, torch.tensor([target])))\n",
        "        self.dataset_index += batch_size\n",
        "        return batch_data, batch_targets\n",
        "\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T13:30:35.209157Z",
          "start_time": "2018-08-21T13:30:34.757103Z"
        },
        "id": "w52KGYlIQT2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98312762-5b5c-4261-ad17-3e9a521e6bdc"
      },
      "source": [
        "print('Número de minibatches de trenamento:', len(loader_train))\n",
        "\n",
        "x_train, y_train = next(iter(loader_train))\n",
        "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
        "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
        "print(\"Tipo das classes das imagens:       \", type(y_train))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de minibatches de trenamento: 20\n",
            "\n",
            "Dimensões dos dados de um minibatch: torch.Size([50, 28, 28])\n",
            "Valores mínimo e máximo dos pixels:  tensor(0.) tensor(1.)\n",
            "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
            "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQA9Zg7GQT2G"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:10.802708Z",
          "start_time": "2018-08-20T21:03:10.793287Z"
        },
        "id": "_8Eg4h_kQT2H"
      },
      "source": [
        "# Escreva aqui o codigo para criar um modelo cujo o equivalente é: \n",
        "# model = torch.nn.Linear(28*28, 10)\n",
        "# model.load_state_dict(dict(weight=torch.zeros(model.weight.shape), bias=torch.zeros(model.bias.shape)))\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import Parameter, Module\n",
        "\n",
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.weight = Parameter(torch.randn(28*28, 10, requires_grad=True))\n",
        "        self.bias = Parameter(torch.randn(10, requires_grad=True))\n",
        "\n",
        "    def print_parameters(self):\n",
        "        print('Weights -', self.weight, self.weight.shape)\n",
        "        print('Biases -', self.bias, self.bias.shape)\n",
        "        print()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.weight) + self.bias"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "#model.load_state_dict(dict(weight=torch.zeros(model.weight.shape), bias=torch.zeros(model.bias.shape)))\n",
        "model.print_parameters()\n",
        "h = model.forward(torch.ones(28*28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLp5VXewrAFA",
        "outputId": "a3d22c1d-4e2a-4729-9304-452aaabe4b7d"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights - Parameter containing:\n",
            "tensor([[-0.7044, -1.9987,  1.3757,  ...,  1.1866,  1.0854,  0.2307],\n",
            "        [-0.0964, -0.7126,  1.3280,  ...,  1.0290,  0.3860,  0.2508],\n",
            "        [ 0.0476,  1.3403, -1.1607,  ..., -1.6446, -0.1817,  0.9301],\n",
            "        ...,\n",
            "        [ 0.2880,  1.4009,  0.0790,  ...,  0.1595, -0.2142,  0.9612],\n",
            "        [ 1.3316,  0.0113, -0.4440,  ..., -1.4287, -0.4731, -1.5490],\n",
            "        [ 2.2793,  0.2803,  0.3863,  ...,  1.4419, -0.2467,  1.3578]],\n",
            "       requires_grad=True) torch.Size([784, 10])\n",
            "Biases - Parameter containing:\n",
            "tensor([ 0.6594, -1.0633,  0.1111, -0.4109,  0.6796,  0.0609, -0.1880, -0.2454,\n",
            "         0.0845, -1.3243], requires_grad=True) torch.Size([10])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NHQB4wGQT2K"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqs2JhJoQT2L"
      },
      "source": [
        "### Inicialização dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:14.146259Z",
          "start_time": "2018-08-20T21:03:14.139515Z"
        },
        "id": "oZuYEkn_QT2M"
      },
      "source": [
        "n_epochs = 50\n",
        "lr = 0.1"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGeMCRw5kQMU"
      },
      "source": [
        "## Definição da Loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcqFzpSTkYtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd74656-adb3-4591-d4f7-119659382920"
      },
      "source": [
        "# Escreva aqui o equivalente de:\n",
        "# criterion = torch.nn.CrossEntropyLoss() = LogSoftmax o NLLLoss.\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class CrossEntropyLoss():\n",
        "    def __call__(self, logits: 'Tensor', target: 'Tensor'):\n",
        "        #to do: one hot de target\n",
        "        softmax = logits.exp() / logits.exp().sum()\n",
        "        logSoftmax = softmax.log()\n",
        "        loss = torch.sum(-target * logSoftmax, dim=-1)\n",
        "        mean_loss = loss.mean(dim=0)\n",
        "        #print('logits', logits)\n",
        "        #print('softmax', softmax, 'sum', softmax.sum())\n",
        "        #print('logSoftmax', logSoftmax, 'mul', logSoftmax.prod())\n",
        "        #print('Negative Log-Likelihood', loss)\n",
        "        return loss\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "criterion(h, 4)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1501.5013, grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição do Optimizer"
      ],
      "metadata": {
        "id": "01BDueU2x4ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escreva aqui o equivalente de:\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "class SGD():\n",
        "    def __init__(self, parameters, learning_rate: float):\n",
        "        self.parameters = parameters\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def step(self):\n",
        "        for param in self.parameters:\n",
        "            param.data -= self.learning_rate * param.grad\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                param.grad *= 0        \n",
        "\n",
        "optimizer = SGD(model.parameters(), lr)"
      ],
      "metadata": {
        "id": "tIlnR_Z8x85O"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXarXeIQT2O"
      },
      "source": [
        "### Laço de treinamento dos parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:40.796410Z",
          "start_time": "2018-08-20T21:03:39.771981Z"
        },
        "id": "L5T_jZZPQT2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "2c7264a7-d500-4958-c437-b3fbce4899f1"
      },
      "source": [
        "epochs = []\n",
        "loss_history = []\n",
        "loss_epoch_end = []\n",
        "total_trained_samples = 0\n",
        "for i in range(n_epochs):\n",
        "    # Substitua aqui o loader_train de acordo com sua implementação do dataloader.\n",
        "    for x_train, y_train in loader_train:\n",
        "        # Transforma a entrada para uma dimensão\n",
        "        inputs = x_train.view(-1, 28 * 28)\n",
        "        # predict da rede\n",
        "        outputs = model(inputs)\n",
        "        print(inputs.shape, outputs.shape, y_train.shape)\n",
        "        print(y_train)\n",
        "\n",
        "        # calcula a perda\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "        # Escreva aqui o código cujo o resultado é equivalente às 3 linhas abaixo:\n",
        "        # optimizer.zero_grad()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        total_trained_samples += x_train.size(0)\n",
        "        epochs.append(total_trained_samples / len(dataset_train))\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    loss_epoch_end.append(loss.item())\n",
        "    print(f'Epoch: {i:d}/{n_epochs - 1:d} Loss: {loss.item()}')\n"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 784]) torch.Size([50, 10]) torch.Size([50])\n",
            "tensor([1, 3, 1, 8, 3, 4, 0, 0, 9, 5, 3, 3, 6, 4, 0, 8, 0, 8, 9, 6, 8, 1, 6, 7,\n",
            "        9, 8, 2, 0, 8, 0, 7, 2, 2, 4, 0, 1, 3, 3, 5, 3, 3, 8, 0, 1, 3, 7, 8, 3,\n",
            "        3, 9])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-184-fa1a50395b5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# calcula a perda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# zero, backpropagation, ajusta parâmetros pelo gradiente descendente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-182-fd91abad0ead>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, logits, target)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlogSoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogSoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print('logits', logits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (10) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dYK0nqsQT2U"
      },
      "source": [
        "### Visualizando gráfico de perda durante o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:52.026752Z",
          "start_time": "2018-08-20T21:03:51.756518Z"
        },
        "id": "LIyUI6AHQT2V"
      },
      "source": [
        "plt.plot(epochs, loss_history)\n",
        "plt.xlabel('época')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLL-GQlKQT2Y"
      },
      "source": [
        "### Visualização usual da perda, somente no final de cada minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:55.246851Z",
          "start_time": "2018-08-20T21:03:54.994428Z"
        },
        "id": "w38EtNxhQT2Z"
      },
      "source": [
        "n_batches_train = len(loader_train)\n",
        "plt.plot(epochs[::n_batches_train], loss_history[::n_batches_train])\n",
        "plt.xlabel('época')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:48.786969Z",
          "start_time": "2018-08-20T21:03:48.781787Z"
        },
        "id": "PiuMsjYtQT2R"
      },
      "source": [
        "# Assert do histórico de losses\n",
        "target_loss_epoch_end = np.array([\n",
        "    1.1979684829711914,\n",
        "    0.867622971534729,\n",
        "    0.7226786613464355,\n",
        "    0.6381281018257141,\n",
        "    0.5809749960899353,\n",
        "    0.5387411713600159,\n",
        "    0.5056464076042175,\n",
        "    0.4786270558834076,\n",
        "    0.4558936357498169,\n",
        "    0.4363219141960144,\n",
        "    0.4191650450229645,\n",
        "    0.4039044976234436,\n",
        "    0.3901679515838623,\n",
        "    0.3776799440383911,\n",
        "    0.3662314713001251,\n",
        "    0.35566139221191406,\n",
        "    0.34584277868270874,\n",
        "    0.33667415380477905,\n",
        "    0.32807353138923645,\n",
        "    0.31997355818748474,\n",
        "    0.312318354845047,\n",
        "    0.3050611615180969,\n",
        "    0.29816246032714844,\n",
        "    0.29158851504325867,\n",
        "    0.28531041741371155,\n",
        "    0.2793029546737671,\n",
        "    0.273544579744339,\n",
        "    0.2680158317089081,\n",
        "    0.26270008087158203,\n",
        "    0.2575823664665222,\n",
        "    0.25264936685562134,\n",
        "    0.24788929522037506,\n",
        "    0.24329163134098053,\n",
        "    0.23884665966033936,\n",
        "    0.23454584181308746,\n",
        "    0.23038141429424286,\n",
        "    0.22634628415107727,\n",
        "    0.22243399918079376,\n",
        "    0.2186385989189148,\n",
        "    0.21495483815670013,\n",
        "    0.21137762069702148,\n",
        "    0.20790249109268188,\n",
        "    0.20452524721622467,\n",
        "    0.20124195516109467,\n",
        "    0.19804897904396057,\n",
        "    0.1949428766965866,\n",
        "    0.19192075729370117,\n",
        "    0.188979372382164,\n",
        "    0.18611609935760498,\n",
        "    0.1833282858133316])\n",
        "\n",
        "assert np.allclose(np.array(loss_epoch_end), target_loss_epoch_end, atol=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fptnTktx_1rh"
      },
      "source": [
        "## Exercício \n",
        "\n",
        "Escreva um código que responda às seguintes perguntas:\n",
        "\n",
        "Qual é a amostra classificada corretamente, com maior probabilidade?\n",
        "\n",
        "Qual é a amostra classificada erradamente, com maior probabilidade?\n",
        "\n",
        "Qual é a amostra classificada corretamente, com menor probabilidade?\n",
        "\n",
        "Qual é a amostra classificada erradamente, com menor probabilidade?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzH8iMGt_1Rr"
      },
      "source": [
        "# Escreva o código aqui:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xmj1GWXUoNs"
      },
      "source": [
        "## Exercício Bonus\n",
        "\n",
        "Implemente um dataloader que aceite como parâmetro de entrada a distribuição probabilidade das classes que deverão compor um batch.\n",
        "Por exemplo, se a distribuição de probabilidade passada como entrada for:\n",
        "\n",
        "`[0.01, 0.01, 0.72, 0.2, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]`\n",
        "\n",
        "Em média, 72% dos exemplos do batch deverão ser da classe 2, 20% deverão ser da classe 3, e os demais deverão ser das outras classes.\n",
        "\n",
        "Mostre também que sua implementação está correta.\n"
      ]
    }
  ]
}